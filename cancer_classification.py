# -*- coding: utf-8 -*-
"""cancer_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E3Mzum7bQQuurHBLfwNZqMk5EImqQUwI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.dummy import DummyClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv('Cancer_Data.csv')

data

data = data.drop(columns=[data.columns[0], data.columns[-1]])

data.describe()

data.duplicated(keep=False)

correlation_matrix = data.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
plt.show()

X = data.iloc[:, 1:]
y = data.iloc[:, 0].to_numpy()

X.info()

X

fig, axes = plt.subplots(nrows=10, ncols=3, figsize=(15, 30))

for i, column in enumerate(X.columns):
    ax = axes[i // 3, i % 3]
    X[column].plot(kind='hist', ax=ax, title=column)
    ax.set_xlabel('Value')
    ax.set_ylabel('Frequency')

plt.tight_layout()
plt.show()

class_counts = data['diagnosis'].value_counts()

plt.figure(figsize=(8, 6))
class_counts.plot(kind='bar', color=['orange', 'royalblue'])
plt.text(0, class_counts[0], str(class_counts[0]), ha='center', va='bottom')
plt.text(1, class_counts[1], str(class_counts[1]), ha='center', va='bottom')
plt.xlabel('Class')
plt.ylabel('Quantity')
plt.show()

scaler = StandardScaler()

X = scaler.fit_transform(X)
X = pd.DataFrame(X)

X.head()

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

df_pca = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])
df_pca['target'] = y

sns.scatterplot(data=df_pca, x='PCA1', y='PCA2', hue='target')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=60)

X_train

y_train

dummy = DummyClassifier(strategy='most_frequent', random_state=60)

dummy.fit(X_train, y_train)

y_pred = dummy.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("Baseline score:", accuracy)

dt_clf = DecisionTreeClassifier(random_state=60)

dt_param_grid = {
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 3, 5]
}

grid_search = GridSearchCV(estimator=dt_clf, param_grid=dt_param_grid, cv=5)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Best params:", best_params)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred))

class_names = ["M", "B"]
cm = confusion_matrix(y_test, y_pred, labels=class_names)
print(cm)
accuracy_score(y_test, y_pred)

plt.imshow(cm, cmap=plt.cm.Blues)
plt.title('Confusion matrix for DecisionTreeClassifier')
plt.colorbar()
plt.xticks([0, 1], class_names)
plt.yticks([0, 1], class_names)
plt.xlabel('Predicted label')
plt.ylabel('True label')

for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')

plt.show()

lg_clf = LogisticRegression(random_state = 60)

lg_param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}

grid_search = GridSearchCV(estimator=lg_clf, param_grid=lg_param_grid, cv=5,
                           scoring='accuracy')

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Best params:", best_params)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred, labels=class_names)
print(cm)
accuracy_score(y_test, y_pred)

plt.imshow(cm, cmap=plt.cm.Blues)
plt.title('Confusion matrix for LogisticRegression')
plt.colorbar()
plt.xticks([0, 1], class_names)
plt.yticks([0, 1], class_names)
plt.xlabel('Predicted label')
plt.ylabel('True label')

for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')

plt.show()

knn_clf = KNeighborsClassifier()

param_grid = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}

grid_search = GridSearchCV(knn_clf, param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Best params:", best_params)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred, labels=class_names)
print(cm)
accuracy_score(y_test, y_pred)

plt.imshow(cm, cmap=plt.cm.Blues)
plt.title('Confusion matrix for KNeighborsClassifier')
plt.colorbar()
plt.xticks([0, 1], class_names)
plt.yticks([0, 1], class_names)
plt.xlabel('Predicted label')
plt.ylabel('True label')

for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')

plt.show()

svm_clf = SVC()

param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}

grid_search = GridSearchCV(svm_clf, param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
print("Best params:", best_params)

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred, labels=class_names)
print(cm)
accuracy_score(y_test, y_pred)

plt.imshow(cm, cmap=plt.cm.Blues)
plt.title('Confusion matrix for KNeighborsClassifier')
plt.colorbar()
plt.xticks([0, 1], class_names)
plt.yticks([0, 1], class_names)
plt.xlabel('Predicted label')
plt.ylabel('True label')

for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')

plt.show()